---
title: "Liotta_Dario_Rlab04"
output: html_document
date: "2024-05-16"
---

```{r}
library(scales)
```

# Exercise 1

***(a) what is the probability distribution of y, the number of times the new method fails to detect the disease ?***

The probability distribution of the number of times the new method fails to detect the disease follows a Binomial distribution, since the phenomenon allows a yes/no answer (binary outcome).

***(b) on the n = 75 patients sample, the new method fails to detect the disease in y = 6 cases. What is the frequentist estimator of the failure probability of the new method ?***

The frequentist estimator for the probability of failure is

$$p_f=\frac{y}{n}$$

```{r}
n <- 75
y <- 6

p_f <- y / n

cat(sprintf("Frequentist estimator of the failure probability: %.0f%%", p_f * 100))
```

***(c) set up a bayesian computation of the posterior probability, assuming a beta distribution with mean value 0.15 and standard deviation 0.14. Plot the posterior distribution for y, and mark on the plot the mean value and variance***

Given the mean $\mu$ and the standard deviation $\sigma$ of the Beta distribution we can evaluate the parameters $\alpha$ and $\beta$ with the following formulas:

$$\alpha=\frac{\mu^2(1-\mu)}{\sigma^2}-\mu \hspace{2cm} \beta=\frac{\mu(1-\mu)^2}{\sigma^2}+\mu-1$$

```{r}
mu    <- 0.15
sigma <- 0.14

alpha_prior <- ((mu^2 * (1-mu)) / (sigma^2)) - mu
beta_prior  <- ((mu * (1-mu)^2) / (sigma^2)) + mu - 1
```

A Beta prior $B(p|\alpha,\beta)$ and a Bernoulli likelihood $Binom(y|n,p)$ will produce a Beta posterior $B(p|\alpha',\beta')$ with

$$\alpha'=\alpha+y \hspace{2cm} \beta'=\beta+n-y$$

```{r}
alpha_posterior <- alpha_prior + y
beta_posterior  <- beta_prior  + n - y
```

Now we can plot and calculate mean and standard deviation.

```{r}
ps <- seq(0, 1, by = 10^(-4))

plot(ps, dbeta(ps, alpha_posterior, beta_posterior),
     type = "l", lwd = 2, col = "blue",
     xlab = "p", ylab = "Probability density",
     main = "Posterior pdf"
    )
grid()

mean_posterior     <- alpha_posterior / (alpha_posterior + beta_posterior)
variance_posterior <- (alpha_posterior * beta_posterior) / ((alpha_posterior + beta_posterior)^2 * (alpha_posterior + beta_posterior + 1))

segments(mean_posterior, 0, mean_posterior, dbeta(mean_posterior, alpha_posterior, beta_posterior), lwd = 2, lty = 2, col = "red")
mtext(expression(bar(p)), side = 1, line = 0.5, at = mean_posterior, col = "red", cex = 1.2)

sigma_value <- mean_posterior + sqrt(variance_posterior)
arrows(mean_posterior, dbeta(sigma_value, alpha_posterior, beta_posterior), sigma_value, dbeta(sigma_value, alpha_posterior, beta_posterior), code = 3, col = "red", lwd = 1.2, length = 0.075)
text((mean_posterior + sigma_value) / 2, dbeta(sigma_value, alpha_posterior, beta_posterior) - 0.8, expression(sigma), col = "red")
```

***(d) Perform a test of hypothesis assuming that if the probability of failing to the detect the disease in ill patients is greater or equal than 15%, the new test is no better that the traditional method. Test the sample at a 5% level of significance in the Bayesian way.***

We want to test the null hypothesis $H_0$, which corresponds to the fact that the probability of failing to detect the disease in ill patients is greater or equal to $15%$:

$$H_0:p\geqslant0.15$$

The alternative hypothesis $H_1$ states the opposite:

$$H_1:p<0.15$$

In order to evaluate the posterior probability of the null hypothesis we need to integrate over the region $[0.15,1]$, and if the result is smaller than $\alpha=0.05$ it means we will reject the null hypothesis.

```{r}
probH0 <- integrate(function(p){dbeta(p, alpha_posterior, beta_posterior)}, lower = 0.15, upper = 1)
cat(sprintf("Posterior probability of the null hypothesis: %.2f", probH0$value))

plot(ps, dbeta(ps, alpha_posterior, beta_posterior),
	   type = 'l', lwd = 2, col = "blue",
     xlab = "p", ylab = "Probability density",
     main = "Posterior pdf",
	   xlim = c(0, 0.3)
    )
grid()

polygon(c(ps[ps >= 0.15], 0.15),
	      c(dbeta(ps, alpha_posterior, beta_posterior)[ps >= 0.15], 0),
        col = "red",
        border = 1,
        text(x = 0.2, y = 2, sprintf("Area = %.2f", probH0$value), col = "red3")
       )
```

Based on the Bayesian hypothesis test, there is sufficient evidence at the 5% significance level to conclude that the probability of failing to detect the disease in ill patients is less than 15%. Hence, the new test is statistically significantly better than the traditional method.

***(e) Perform the same hypothesis test in the classical frequentist way.***

Let's evaluate the p-value:

```{r}
p_value <- pbinom(y, n, 0.15)
cat(sprintf("P-value: %.2f%%", p_value * 100))
```

We can also visualize the result:

```{r}
probabilities <- pbinom(seq(0, 8, 1), n, 0.15)

colors    <- rep("lightblue", length(probabilities))
colors[7] <- "red"

barplot(probabilities,
        ylim = c(0, 0.23),
        main = "Cumulative binomial distirbution",
        xlab = "y",
        ylab = "Cumulative probability",
        col = colors,
        border = "darkblue",
        names.arg = seq(0, 8, 1),
        las = 1,
        cex.names = 0.8,
        cex.axis = 0.8,
        cex.lab = 0.8,
        cex.main = 0.9
)
grid(nx = NULL, ny = NULL, col = "gray", lty = "dotted")

text(seq(1, length(probabilities), 1), probabilities + 0.01, 
     labels = round(probabilities, 3), 
     cex = 0.7, 
     col = "blue")

abline(h = 0.05, col = "orange", lty = "dashed", lwd = 2)

legend("topleft", legend = "Acceptance", lty = "dashed", lwd = 2, col = "orange")
```

In this case the null hypothesis can't be rejected, so with this approach we can't say if the new test is better than the old one.

# Exercise 2

***(a) find the posterior distribution, the posterior mean and standard deviation***

The likelihood is given by the product of the gaussians function:

$$f\left(x_1,x_2,\dots, x_n\right|\mu)\propto e^{-\frac{\left(x_1-\mu\right)^2}{2\sigma^2}}\cdot e^{-\frac{\left(x_2-\mu\right)^2}{2\sigma^2}}\cdot\dotsc\cdot e^{-\frac{\left(x_n-\mu\right)^2}{2\sigma^2}}$$

Summing all the expontents:

$$-\frac{1}{2\sigma^2}\left[\left(x_1-\mu\right)^2+\left(x_2-\mu\right)^2+\dotsc+\left(x_n-\mu\right)^2\right]=-\frac{1}{2\sigma^2}\left[x_1^2-2\mu x_1+\mu^2+\dotsc+x_n^2-2\mu x_n+\mu^2\right]$$

So the exponent will be:

$$-\frac{1}{2\sigma^2}\left[\left(x_1^2+\dotsc x_n^2\right)-2\mu\left(x_1+\dotsc x_n\right)+n\mu^2\right]=-\frac{n}{2\sigma^2}\left[\frac{x_1^2+\dotsc x_n^2}{n}-2\mu\bar{x}+\mu^2\right]$$

Summing and subtracting $\bar{y}^2$:

$$-\frac{n}{2\sigma^2}\left[\frac{x_1^2+\dotsc x_n^2}{n}-\bar{y}^2+\bar{y}^2-2\mu\bar{x}+\mu^2\right]=-\frac{n}{2\sigma^2}\left[\frac{x_1^2+\dotsc x_n^2}{n}-\bar{y}^2\right]-\frac{n}{2\sigma^2}\left(\bar{y}-\mu\right)^2$$

Considering only the term dependent by $\mu$:

$$f\left(x_1,x_2,\dots, x_n\right|\mu)\propto e^{-\frac{\left(\bar{y}-\mu\right)^2}{2\left(\frac{\sigma}{\sqrt{n}}\right)^2}}$$

which corresponds to a gaussian where $\bar{y}$ is the variable, $\mu$ is the mean value and $\frac{\sigma}{\sqrt{n}}$ the standard deviation.

Having identified the likelihood, we can now proceed with the calculations.

```{r}
# Fixing the precision of calculations
precision <- 10^(-5)

# Data
data <- c(4.09, 4.68, 1.87, 2.62, 5.58, 8.68, 4.07, 4.78, 4.79, 4.49, 5.85, 5.09, 2.40, 6.27, 6.30, 4.47)

# Prior function
prior <- function(mu) {
  if(mu == 0) {
    mu <- precision
  }
  
  if(mu > 0 && mu <= 3) {
    return(mu)
  }
  else if(mu > 3 && mu <= 5) {
    return(3)
  }
  else if(mu > 5 && mu <= 8) {
    return(8 - mu)
  }
  else if(mu > 8) {
    return(0)
  }
}

# Posterior function
posterior <- function(mu) {
  variance <- 4
  return(prior(mu) * dnorm(mean(data), mean = mu, sd = sqrt(variance / length(data))))
}

# x values
mus <- seq(0, 12, by = precision)

# Posterior values
vs  <- double(length(mus))
for(i in 1:length(mus)) {
  vs[i] <- posterior(mus[i])
}
vs <- vs / sum(vs)   # Normalization

# Plot
plot(mus, vs, 
     type = "l", lwd = 2, col = "blue",
     xlab = expression(mu), ylab = "Probability density",
     main = "Posterior pdf"
    )
grid()

# Posterior mean
posterior_mean <- sum(mus * vs)
cat(sprintf("Posterior mean: %.3f\n", posterior_mean))

# Posterior standard deviation
posterior_variance <- sum((mus - posterior_mean)^2 * vs)
cat(sprintf("Posterior standard deviation: %.3f\n", sqrt(posterior_variance)))
```

***(b) find the 95% credibility interval for Î¼***

```{r}
# Integral values with cumsum() function
integral_values <- cumsum(vs)

value_of_025 <- mus[which.min(abs(integral_values - 0.025))]
value_of_975 <- mus[which.min(abs(integral_values - 0.975))]

cat(sprintf("95%% credibility interval: [%.6f, %.6f]", value_of_025, value_of_975))
```

***(c) plot the posterior distribution, indicating on the same plot: the mean value, the standard deviation, and the 95% credibility interval***

```{r}
# Plot of the function
plot(mus, vs, 
     type = "l", lwd = 2, col = "blue",
     xlab = expression(mu), ylab = "Probability density",
     main = "Posterior pdf"
    )
grid()

# Plot of the mean value
segments(posterior_mean, 0, posterior_mean, vs[which.min(abs(mus - posterior_mean))], lwd = 2, lty = 2, col = "red")
mtext(expression(bar(mu)), side = 1, line = 0.5, at = posterior_mean, col = "red", cex = 1.2)

# Plot of the standard deviation
x_value_sigma <- which.min(abs(mus - (posterior_mean + sqrt(posterior_variance))))
arrows(posterior_mean, vs[x_value_sigma], mus[x_value_sigma], vs[x_value_sigma], code = 3, col = "red", lwd = 1.2, length = 0.075)
text((posterior_mean + mus[x_value_sigma]) / 2, vs[x_value_sigma] - 5*10^(-7), expression(sigma), col = "red")

# Plot of the 95% credibility interval
area_x <- c(value_of_025, mus[mus >= value_of_025 & mus <= value_of_975], value_of_975)
area_y <- c(0, vs[mus >= value_of_025 & mus <= value_of_975], 0)
polygon(area_x, area_y, col = alpha("blue", 0.25))
legend("topright", legend = "95% credibility interval", fill = alpha("blue", 0.25), border = "black")
```

***(d) plot, on the same graph, the prior, the likelihood and the posterior distribution***

```{r}
# Prior values
prior_values <- double(length(mus))
for(i in 1:length(mus)) {
  prior_values[i] <- prior(mus[i])
}
prior_values <- prior_values / sum(prior_values)   # Normalization

#Plot of the posterior
plot(mus, vs, 
     type = "l", lwd = 2, col = "blue",
     xlab = expression(mu), ylab = "Probability density",
     main = "Probability density functions"
    )
grid()

# Plot of the prior
lines(mus, prior_values, lwd = 2, col = "red")

#Plot of the likelihood
variance          <- 4
likelihood_values <- sapply(mus, dnorm, x = mean(data), sd = sqrt(variance / length(data)))
likelihood_values <- likelihood_values / sum(likelihood_values)
lines(mus, likelihood_values, lwd = 2, col = "green")

legend("topright", legend = c("Prior", "Likelihood", "Posterior"), 
       col = c("red", "green", "blue"), lwd = 2)
```

# Exercise 3

```{r}
# Function of the toy model
six_boxes_toy_model <- function(n_boxes, n_extractions) {
  
  # Getting the vector of names (H0, H1, H2, ...)
  box_names <- character(n_boxes)
  
  for(i in 1:n_boxes) {
    box_names[i] <- paste("H", i-1, sep = "")
  }
  
  # Extraction of the box
  box <- sample(0:(n_boxes-1), 1)
  
  # Extractions of the balls from the chosen box
  extractions <- rbinom(n_extractions, 1, box / (n_boxes - 1))
  
  # Fixing the matrix of probabilities: columns are boxes, rows are extractions
  p <- matrix(0, ncol = n_boxes, nrow = n_extractions + 1)
  p[1,] <- 1/n_boxes   # The first row represent the prior probability, so 1/6 in our case
  
  # I choose outcome 1 as extraction of a white ball and outcome 0 as extraction of a black ball
  ps_white <- seq(0, 5) / (n_boxes - 1)
  ps_black <- 1 - ps_white
  
  cat(sprintf("Box chosen: H%d\n\n", box))
  cat(paste("Probabilities:\n"))

  # Update of the probabilities with extractions
  for(i in 1:n_extractions) {
    
    if(extractions[i] == 0) {
      v <- ps_black * p[i,]
    }
    
    else if(extractions[i] == 1) {
      v <- ps_white * p[i,]
    }
    
    p[i+1,] <- v / sum(v)   # Normalization
    
    cat(sprintf("After extraction %d: ", i))
    
    for(j in 1:n_boxes) {
      cat(sprintf("%s -> %.2f%%   ", box_names[j], p[i+1,j] * 100))
    }
    
    cat(sprintf("\n"))
  }
  
  # Plots
  colors = c("red", "blue", "yellow", "green", "orange", "brown")
  
  plot(seq(0, n_extractions), p[,1],
       type = "l", lwd = 2, col = colors[1],
       main = "Probability change with extractions",
       xlab = "Number of extraction",
       ylab = "Probability",
       xlim = c(0, n_extractions+7),
       ylim = c(0, 1.1)
      )
  grid()
  
  for(j in 2:ncol(p)) {
    lines(seq(0, n_extractions), p[,j], lwd = 2, col = colors[j])
  }
  
  legend("right", legend = box_names, col = colors, lwd = 2)
    
}

six_boxes_toy_model(n_boxes = 6, n_extractions = 50)
```
